{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruções\n",
    "- Redes Neuronais + Backpropagation\n",
    "\n",
    "Neste trabalho você irá implementar uma rede neuronal com três camadas:\n",
    "\n",
    "1. Camada de entrada: cada unidade representa uma dimensão do dado de entrada.\n",
    "\n",
    "2. Camada oculta: cada unidade representa uma transformação a partir das unidades de entrada.\n",
    "\n",
    "3. Camada de saída: cada unidade representa a chance da saída correspondente ser a correta.\n",
    "\n",
    "Você irá utilizar a função Sigmóide para obter não-linearidade. Além disso, a função de perda a ser minimizada é a seguinte:\n",
    "\n",
    "![title](formula.jpg)\n",
    "\n",
    "onde m é a quantidade de entradas no treino, K é o número de saídas possíveis,  representa a saída correta de cada classe k em cada entrada (i), e similarmente representa a saída dada pela rede neuronal.\n",
    "\n",
    "O dado a ser utilizado está anexado. Trata-se de 5000 entradas, onde cada entrada refere-se a um dígito escrito manualmente (i.e., MNIST dataset). Dessa forma, m=5000 e K=10. Cada entrada é dada por uma matriz de dimensões 28 por 28, ou seja, um vetor de 784 dimensões. A primeira coluna do arquivo sempre é o rótulo do dígito correto.\n",
    "\n",
    "A rede neuronal a ser implementada deverá ter 784 unidades de entrada e 10 unidades de saída. Em seus experimentos, você deverá variar o número de unidades na camada oculta (25, 50, 100).\n",
    "\n",
    "Além disso, você deverá comparar os seguintes algoritmos de cálculo de gradiente:\n",
    "\n",
    "1. Gradient Descent: o gradiente é calculado após cada época (após as 5000 entradas serem processadas).\n",
    "\n",
    "2. Stochastic Gradient Descent: o gradiente é calculado após cada entrada.\n",
    "\n",
    "3. Mini-Batch: o gradiente é calculado após um certo número de entradas (considere 10 e 50).\n",
    "\n",
    "Por fim, você também deverá variar a taxa de aprendizado: 0.5, 1, 10.\n",
    "\n",
    "O documento a ser entregue deverá apresentar o resultado de seus experimentos. Ou seja, deverá apresentar discussão da variação do número de unidades na camada oculta para cada um dos três algoritmos de cálculo de gradiente. Você deverá apresentar gráficos mostrando a convergência do erro empírico para cada situação (unidades na camada oculta, algoritmo de cálculo do gradiente, taxa de aprendizado). Você deverá deixar claras todas as hipóteses que julgar serem pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "import numpy\n",
    "import keras\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando Arquivo\n",
    "Carregando o arquivo com os 5000 digitos e fazendo uma divisão de folds para fazer o cross-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"/home/lfmendes/data/mestrado/machine-learning/ml-neural/data_tp1.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,1:]\n",
    "y = dataset[:,0]\n",
    "\n",
    "y_cat = np_utils.to_categorical(y)\n",
    "num_classes = y_cat.shape[1]\n",
    "\n",
    "num_pixels = 784\n",
    "\n",
    "print(\"Número de entradas: %s\" % (len(y)))\n",
    "print(\"Número de features: %s\" % (len(X[0])))\n",
    "print(\"Número de classes: %s\" % (num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando um divisão de 5 folds stratified, isto é, mantendo o número de classes por fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código para criação do modelo dado os parâmetros que podem ser variados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate=0.5,neuronios_ocultos=25):\n",
    "    # create model   \n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, activation='relu', input_shape=(784,)))\n",
    "    model.add(Dense(neuronios_ocultos, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    sgd = optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])    \n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo parâmetros de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes=[1,10,50,5000]\n",
    "learning_rates=[0.5, 1, 10]\n",
    "neuronios=[25,50,100]\n",
    "epochs_variations=[20,100,250,500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo experimentação\n",
    "Dividindo em treino e teste usando 5 kfolds.\n",
    "Para cada um dos parâmetros, será feito testes nos 5 folds e o resultado será a média da métrica escolhida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "def get_folds(seed,fold_size=5):    \n",
    "    return StratifiedKFold(n_splits=fold_size, random_state=seed,shuffle=True)\n",
    "\n",
    "def experimentation(batch_size=100,learning_rate=0.5,neuronio=100,epochs=100,seed=42, verbose=0):    \n",
    "    fold = 0\n",
    "    avg_error = 0.0\n",
    "    avg_acc = 0.0\n",
    "    first = True\n",
    "    accs = []\n",
    "    \n",
    "    skf = get_folds(seed)\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        print(\"Fazendo teste nos fold \" + str(fold))\n",
    "        print(\"batch_size: \" + str(batch_size))\n",
    "        print(\"learning_rate: \" + str(learning_rate))\n",
    "        print(\"neuronio: \" + str(neuronio))\n",
    "        print(\"epochs: \" + str(epochs))\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        X_train = X_train.reshape(len(train_index), 784)\n",
    "        X_test = X_test.reshape(len(test_index), 784)\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        X_train /= 255\n",
    "        X_test /= 255\n",
    "        print(X_train.shape[0], 'train samples')\n",
    "        print(X_test.shape[0], 'test samples')\n",
    "\n",
    "        y_train = np_utils.to_categorical(y_train)\n",
    "        y_test = np_utils.to_categorical(y_test)   \n",
    "        \n",
    "        print('Distribuicao no treino: ' + str(y_train.sum(axis=0, dtype='float')))\n",
    "        print('Distribuicao no test: ' + str(y_test.sum(axis=0, dtype='float')))\n",
    "\n",
    "        model=create_model(learning_rate=learning_rate,neuronios_ocultos=neuronio)\n",
    "        \n",
    "        if(first):\n",
    "            model.summary()\n",
    "            first = False\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test), show_accuracy=True,\n",
    "                  epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "                  shuffle=True)\n",
    "\n",
    "        # Final evaluation of the model\n",
    "        scores = model.evaluate(X_train, y_train, verbose=1)\n",
    "        print(\"Resultado no Treino\")\n",
    "        print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
    "        print(\"%s: %.2f%%\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "        \n",
    "        scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "        print(\"Resultado no Teste\")\n",
    "        print('Distribuicao na predicao: ' + str(model.predict(X_test).sum(axis=0, dtype='float')))\n",
    "        print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
    "        print(\"%s: %.2f%%\\n\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "        # TODO remover \n",
    "        if ((scores[1]*100) < 20):\n",
    "            print('deu ruim')\n",
    "            return train_index,test_index\n",
    "        \n",
    "        avg_acc = avg_acc + scores[1]*100\n",
    "        accs.append(scores[1]*100) \n",
    "\n",
    "        fold=fold+1\n",
    "\n",
    "    print(avg_acc/fold_size) \n",
    "    print(accs)\n",
    "    print('\\n\\n   ---------------------  \\n\\n')\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentando com batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes=[500,5000]\n",
    "iterations=[120,1111]\n",
    "\n",
    "batches_results = {}\n",
    "for batch in batch_sizes:\n",
    "    print('Experimentando com batch size igual a ' + str(batch))\n",
    "    print('\\n\\n')\n",
    "    batches_results[str(batch)] = []\n",
    "    for it in iterations: \n",
    "        batches_results[str(batch)].extend(experimentation(batch_size=batch,seed=it))\n",
    "    print(batches_results)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(batches_results)\n",
    "plt.bar(list(batches_results.keys()), batches_results.values(), color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentando com learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates_results = {}\n",
    "for learning_rate in learning_rates:\n",
    "    print('Experimentando com learning_rates igual a ' + str(learning_rate))\n",
    "    learning_rates_results[str(learning_rate)] = experimentation(learning_rate=learning_rate)    \n",
    "\n",
    "plt.bar(list(learning_rates_results.keys()), learning_rates_results.values(), color='g')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentando com neuronios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronios_results = {}\n",
    "for neuronio in neuronios:\n",
    "    print('Experimentando com learning_rates igual a ' + str(neuronio))\n",
    "    neuronios_results[str(neuronio)] = experimentation(neuronio=neuronio)    \n",
    "\n",
    "plt.bar(list(neuronios_results.keys()), neuronios_results.values(), color='g')\n",
    "plt.show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
