{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruções\n",
    "- Redes Neuronais + Backpropagation\n",
    "\n",
    "Neste trabalho você irá implementar uma rede neuronal com três camadas:\n",
    "\n",
    "1. Camada de entrada: cada unidade representa uma dimensão do dado de entrada.\n",
    "\n",
    "2. Camada oculta: cada unidade representa uma transformação a partir das unidades de entrada.\n",
    "\n",
    "3. Camada de saída: cada unidade representa a chance da saída correspondente ser a correta.\n",
    "\n",
    "Você irá utilizar a função Sigmóide para obter não-linearidade. Além disso, a função de perda a ser minimizada é a seguinte:\n",
    "\n",
    "![title](formula.jpg)\n",
    "\n",
    "onde m é a quantidade de entradas no treino, K é o número de saídas possíveis,  representa a saída correta de cada classe k em cada entrada (i), e similarmente representa a saída dada pela rede neuronal.\n",
    "\n",
    "O dado a ser utilizado está anexado. Trata-se de 5000 entradas, onde cada entrada refere-se a um dígito escrito manualmente (i.e., MNIST dataset). Dessa forma, m=5000 e K=10. Cada entrada é dada por uma matriz de dimensões 28 por 28, ou seja, um vetor de 784 dimensões. A primeira coluna do arquivo sempre é o rótulo do dígito correto.\n",
    "\n",
    "A rede neuronal a ser implementada deverá ter 784 unidades de entrada e 10 unidades de saída. Em seus experimentos, você deverá variar o número de unidades na camada oculta (25, 50, 100).\n",
    "\n",
    "Além disso, você deverá comparar os seguintes algoritmos de cálculo de gradiente:\n",
    "\n",
    "1. Gradient Descent: o gradiente é calculado após cada época (após as 5000 entradas serem processadas).\n",
    "\n",
    "2. Stochastic Gradient Descent: o gradiente é calculado após cada entrada.\n",
    "\n",
    "3. Mini-Batch: o gradiente é calculado após um certo número de entradas (considere 10 e 50).\n",
    "\n",
    "Por fim, você também deverá variar a taxa de aprendizado: 0.5, 1, 10.\n",
    "\n",
    "O documento a ser entregue deverá apresentar o resultado de seus experimentos. Ou seja, deverá apresentar discussão da variação do número de unidades na camada oculta para cada um dos três algoritmos de cálculo de gradiente. Você deverá apresentar gráficos mostrando a convergência do erro empírico para cada situação (unidades na camada oculta, algoritmo de cálculo do gradiente, taxa de aprendizado). Você deverá deixar claras todas as hipóteses que julgar serem pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolução\n",
    "\n",
    "Para esse trabalho prático utilizarei a biblioteca KERAS para criar o modelo de rede neuronal proposto. Além disso, usaremos a biblioteca numpy e pandas para tratar dados vetoriais e para plotar gráficos dos resultados obtidos.\n",
    "\n",
    "Os comentários e conclusões estão presentes juntamente com o código desse notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "import numpy\n",
    "import keras\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando Arquivo\n",
    "Carregando o arquivo com os 5000 digitos e dividindo em X (matrix com as features) e y (vetor com as respostas)\n",
    "\n",
    "Além disso, definimos o número de categorias(classes) baseados no y e o número de pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"/home/lfmendes/data/mestrado/machine-learning/ml-neural/data_tp1.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,1:]\n",
    "y = dataset[:,0]\n",
    "\n",
    "y_cat = np_utils.to_categorical(y)\n",
    "num_classes = y_cat.shape[1]\n",
    "\n",
    "num_pixels = 784\n",
    "\n",
    "print(\"Número de entradas: %s\" % (len(y)))\n",
    "print(\"Número de features: %s\" % (len(X[0])))\n",
    "print(\"Número de classes: %s\" % (num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Código para criação do modelo dado os parâmetros que podem ser variados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função a seguir irá definir o modelo que será utilizado, dando espaço para modificar os parâmetros pedidos, no caso definimos os seguintes parâmetros:\n",
    "\n",
    "* learning_rate - Taxa de aprendizado\n",
    "* neuronios_ocultos - Quantidade de neuronios na camada oculta\n",
    "\n",
    "O modelo a ser cosntruído é composto de\n",
    "* uma camada de entrada com 784 neuronios e utiliza a função de ativação 'relu'.\n",
    "* uma camada oculta com o número de neurônios parametrizada pela função\n",
    "* uma camada de saída com número de neurônios igual ao número de classes, nesse caso utilizamos a função softmax \n",
    "\n",
    "Para fazer a otimização do modelo, estamos usando um SGD com learning rate definido no parâmetro da função. Importante salientar que durante o **fit** se utilizarmos o SGD com batch size igual a 1 ele se comporta como Stochastic Gradient Descent (o gradiente é calculado após cada entrada), se usarmos o batch size igual ou maior ao tamanho do treino ele se comporta como Gradient Descent (o gradiente é calculado após cada época) e nos outros casos ele funciona como Mini-Batch: o gradiente é calculado após um certo número de entradas.\n",
    "\n",
    "A função de perda utilizada é a **categorical_crossentropy** e a métrica utilizada é a **acurácia**.\n",
    "\n",
    "![alt text](rede.png \"Desenho da Rede\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate=0.5,neuronios_ocultos=25):\n",
    "    # create model   \n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, activation='relu', input_shape=(784,)))\n",
    "    model.add(Dense(neuronios_ocultos, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    sgd = optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])    \n",
    "   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo experimentação\n",
    "Dividindo em treino e teste usando 5 kfolds.\n",
    "\n",
    "Na próxima célula definimos uma função que separa em folds, sendo que para isso ela aleatoriza a entrada e utiliza uma semente aleatória. Além disso, por estarmos utilizando a função **StratifiedKFold** da biblioteca **sklearn** então os folds irão ser criados de forma a respeitar a dsitribuição de classes da entrada\n",
    "\n",
    "Já a função **experimentation()** é a principal desse trabalho, nela fazemos \n",
    "* a divisão de folds da entrada\n",
    "* o tratamento da entrada, passando os valores brutos para floats \n",
    "* transformamos nossa entrada em vetorial ou categórica, isto é, um valor como 2 vira [0,1,0,0,0,0,0,0,0,0]\n",
    "* Criamos o modelo como já definido na função **create_model()**\n",
    "* Fazemos o fit desse modelo utilizando apenas os dados de treino\n",
    "* Calculamos e guardamos a acurácia obtida nos dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "fold_size = 5\n",
    "\n",
    "def get_folds(seed,fold_size=5):    \n",
    "    return StratifiedKFold(n_splits=fold_size, random_state=seed,shuffle=True)\n",
    "\n",
    "def experimentation(batch_size=5000,learning_rate=0.5,neuronio=100,epochs=250,seed=42, verbose=0):    \n",
    "    fold = 0\n",
    "    avg_error = 0.0\n",
    "    avg_acc = 0.0\n",
    "    first = True\n",
    "    accs = []\n",
    "    \n",
    "    skf = get_folds(seed)\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        print(\"Fazendo teste nos fold \" + str(fold))\n",
    "        if(verbose > 0):            \n",
    "            print(\"batch_size: \" + str(batch_size))\n",
    "            print(\"learning_rate: \" + str(learning_rate))\n",
    "            print(\"neuronio: \" + str(neuronio))\n",
    "            print(\"epochs: \" + str(epochs))\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        X_train = X_train.reshape(len(train_index), 784)\n",
    "        X_test = X_test.reshape(len(test_index), 784)\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        X_train /= 255\n",
    "        X_test /= 255\n",
    "        \n",
    "        if(verbose > 0):\n",
    "            print(X_train.shape[0], 'train samples')\n",
    "            print(X_test.shape[0], 'test samples')\n",
    "\n",
    "        y_train = np_utils.to_categorical(y_train)\n",
    "        y_test = np_utils.to_categorical(y_test)   \n",
    "        \n",
    "        if(verbose > 0):\n",
    "            print('Distribuicao no treino: ' + str(y_train.sum(axis=0, dtype='float')))\n",
    "            print('Distribuicao no test: ' + str(y_test.sum(axis=0, dtype='float')))\n",
    "\n",
    "        model=create_model(learning_rate=learning_rate,neuronios_ocultos=neuronio)\n",
    "        \n",
    "        if((verbose > 0) and first):\n",
    "            model.summary()\n",
    "            first = False\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                  epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
    "                  shuffle=True)\n",
    "\n",
    "        # Final evaluation of the model        \n",
    "        if(verbose > 0):\n",
    "            # Avaliando no Treino (verificar caso de overfitting)\n",
    "            scores = model.evaluate(X_train, y_train, verbose=1)\n",
    "            print(\"Resultado no Treino\")\n",
    "            print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
    "            print(\"%s: %.2f%%\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "        \n",
    "        # Avaliando o Teste\n",
    "        scores = model.evaluate(X_test, y_test, verbose=verbose)        \n",
    "        if(verbose > 0):\n",
    "            print(\"Resultado no Teste\")\n",
    "            print('Distribuicao na predicao: ' + str(model.predict(X_test).sum(axis=0, dtype='float')))\n",
    "            print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n",
    "            print(\"%s: %.2f%%\\n\\n\" % (model.metrics_names[1], scores[1]*100))\n",
    "        \n",
    "        avg_acc = avg_acc + scores[1]*100\n",
    "        accs.append(scores[1]*100) \n",
    "\n",
    "        fold=fold+1\n",
    "\n",
    "    print(avg_acc/fold_size) \n",
    "    print(accs)\n",
    "    print('\\n\\n   ---------------------  \\n\\n')\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo parâmetros de teste\n",
    "\n",
    "Para realizar os testes requisitados no trabalho, vamos definir alguams variáveis contendo os valores possíveis para cada parâmetro.\n",
    "\n",
    "* batch size - Variando entre um SGD puro e o Gradient Descent\n",
    "* learning rate - Variando as taxas de aprendizado\n",
    "* neuronios - Variando a quantidade de neurônios na camada oculta\n",
    "* epochs - Variando a quantidade de épocas \n",
    "\n",
    "Além disso, definimos 5 sementes para serem utilizadas durante a experimentação, sendo assim para cada valor de parâmetro teremos 25 resultados sendo 5 de cada fold para cada semente. Isto foi feito para aproximarmos melhor dos valores reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes=[1,10,50,5000]\n",
    "learning_rates=[0.5, 1, 10]\n",
    "neuronios=[25,50,100]\n",
    "epochs_variations=[20,100,250,500]\n",
    "\n",
    "# Seeds utilizadas para rodar vários experimentos e termos um valor mais aproximado do \"real\"\n",
    "iterations=[120,1111,422,122,8999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizando o resultado\n",
    "\n",
    "Para facilitar as conclusões vamos definir uma função que mostra o histograma com as médias para cada parâmetro e o boxplot com esses valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def print_results(results):\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df)\n",
    "    plt.bar(list(df.mean().keys()), df.mean(), color='g')\n",
    "    plt.show()\n",
    "    df.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentando com batch size\n",
    "\n",
    "O objetivo desses experimentos é verificar qual o impacto do batch_size no resultado. \n",
    "\n",
    "Nesse caso estamos realizando os seguintes experimentos\n",
    "\n",
    "1. Gradient Descent: o gradiente é calculado após cada época (após as 5000 entradas serem processadas).\n",
    "\n",
    "2. Stochastic Gradient Descent: o gradiente é calculado após cada entrada.\n",
    "\n",
    "3. Mini-Batch: o gradiente é calculado após um certo número de entradas (considere 10 e 50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_results = {}\n",
    "for batch in batch_sizes:\n",
    "    print('Experimentando com batch size igual a ' + str(batch))\n",
    "    print('\\n\\n')\n",
    "    batches_results[str(batch)] = []\n",
    "    for it in iterations: \n",
    "        batches_results[str(batch)].extend(experimentation(batch_size=batch,seed=it))\n",
    "    print(batches_results)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora para visualizar melhor os resultados vamos plotar um histograma com as médias de cada valor e depois um boxplot para termos uma noção da variação do resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(batches_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados do experimento com batch size\n",
    "\n",
    "É fácil notar pelo gráfico que batch size......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentando com learning_rates\n",
    "\n",
    "O objetivo desses experimentos é avaliar qual o impacto da taxa de aprendizado.\n",
    "Nesse caso vamos testar as seguintes taxas de aprendizado: [0.5, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates_results = {}\n",
    "for learning_rate in learning_rates:\n",
    "    print('Experimentando com learning rate igual a ' + str(learning_rate))\n",
    "    print('\\n\\n')\n",
    "    learning_rates_results[str(learning_rate)] = []\n",
    "    for it in iterations: \n",
    "        learning_rates_results[str(learning_rate)].extend(experimentation(learning_rate=learning_rate, seed=it))\n",
    "    print(learning_rates_results)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(learning_rates_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados do experimento com learning rate\n",
    "\n",
    "É fácil notar pelo gráfico que learning rate......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentando com neuronios na camada oculta\n",
    "\n",
    "O objetivo desses experimentos é avaliar qual o impacto do número de neurônios na camada oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuronios_results = {}\n",
    "for neuronio in neuronios:\n",
    "    print('Experimentando com learning rate igual a ' + str(neuronio))\n",
    "    print('\\n\\n')\n",
    "    neuronios_results[str(neuronio)] = []\n",
    "    for it in iterations: \n",
    "        neuronios_results[str(neuronio)].extend(experimentation(neuronio=neuronio,seed=it))\n",
    "    print(neuronios_results)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(neuronios_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados do experimento com neuronios na camada oculta\n",
    "\n",
    "É fácil notar pelo gráfico que batch size......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variando o número de camadas ocultas para cada gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "neuroniosXbatch_results = {}\n",
    "for x in itertools.product(batch_sizes, neuronios):    \n",
    "    neuronio = x[1]\n",
    "    batch = x[0]\n",
    "    print('Experimentando com  (batch,neuronio)' + str(x))\n",
    "    print('\\n\\n')\n",
    "    neuroniosXbatch_results[str(x)] = []    \n",
    "    neuroniosXbatch_results[str(neuronio)].extend(experimentation(neuronio=neuronio,batch_size=batch))\n",
    "    print(neuroniosXbatch_results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(neuroniosXbatch_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ao final testar\n",
    "\n",
    "* Rodar uma vez com 70-30 e a melhor opção de parâmetros encontrados\n",
    "* * mostrar o historico por epoch\n",
    "* Adicionar dropout e verificar se o resultado melhor\n",
    "* * mostrar o treino vs teste nesse caso\n",
    "* conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
